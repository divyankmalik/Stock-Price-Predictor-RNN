# -*- coding: utf-8 -*-
"""stock_price_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YmUtf43fC2LjecsFjgGBExqpJqZheZ_b
"""

!pip install kaggle

!rm -rf ~/.kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d jakewright/9000-tickers-of-stock-market-data-full-history

!unzip 9000-tickers-of-stock-market-data-full-history.zip

import pandas as pd
df = pd.read_csv('all_stock_data.csv')
df.head(10)

import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Input
    from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback
except ImportError:
    print("Please install tensorflow: pip install tensorflow")

df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values(['Ticker', 'Date'])

print(f"Dataset shape: {df.shape}")
print(f"Date range: {df['Date'].min()} to {df['Date'].max()}")
print(f"Number of unique tickers: {df['Ticker'].nunique()}")

df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values(['Ticker', 'Date'])

print(f"Dataset shape: {df.shape}")
print(f"Date range: {df['Date'].min()} to {df['Date'].max()}")
print(f"Number of unique tickers: {df['Ticker'].nunique()}")

def create_sequences(data, seq_length):
    """Create sequences for time series prediction"""
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length, 0])  # Predict Close price
    return np.array(X), np.array(y)

ticker_to_predict = 'MSI'
seq_length = 60

stock_df = df[df['Ticker'] == ticker_to_predict].copy()
stock_df = stock_df.sort_values('Date').reset_index(drop=True)

# Select features
features = ['Close', 'Open', 'High', 'Low', 'Volume']
data = stock_df[features].values

print(f"\nSelected ticker: {ticker_to_predict}")
print(f"Total samples: {len(data)}")
print(f"Features used: {features}")

scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data)

print(f"Original Close price range: ${data[:, 0].min():.2f} - ${data[:, 0].max():.2f}")
print(f"Scaled Close price range: {data_scaled[:, 0].min():.4f} - {data_scaled[:, 0].max():.4f}")

"""**bold text**FIRST TRY TO CREATE A MODEL

"""

X, y = create_sequences(data_scaled, seq_length)

print(f"X shape: {X.shape} (samples, timesteps, features)")
print(f"y shape: {y.shape} (samples,)")

train_size = int(len(X) * 0.8)

X_train = X[:train_size]
y_train = y[:train_size]

X_test = X[train_size:]
y_test = y[train_size:]

print(f"\nüìä Data split:")
print(f"Training set: {X_train.shape[0]} samples ({80}%)")
print(f"Test set:     {X_test.shape[0]} samples ({20}%)")

class StockRNN(keras.Model):
    """Custom RNN model with explicit forward pass"""

    def __init__(self, input_shape, lstm_units=50, embedding_dim=32):
        super(StockRNN, self).__init__()

        # Feature embedding - projects input features to higher dimensional space
        self.embedding = Dense(embedding_dim, activation='relu', name='feature_embedding')

        # LSTM layers
        self.lstm1 = LSTM(lstm_units, return_sequences=True, name='lstm_layer_1')
        self.dropout1 = Dropout(0.2, name='dropout_1')

        self.lstm2 = LSTM(lstm_units, return_sequences=True, name='lstm_layer_2')
        self.dropout2 = Dropout(0.2, name='dropout_2')

        self.lstm3 = LSTM(lstm_units, return_sequences=False, name='lstm_layer_3')
        self.dropout3 = Dropout(0.2, name='dropout_3')

        # Dense layers
        self.dense1 = Dense(25, activation='relu', name='dense_1')
        self.output_layer = Dense(1, name='output')

    def call(self, inputs, training=False):
        """
        Forward pass through the network
        This is where data flows through each layer
        """
        # Embedding: Transform raw features to learned representation
        x = self.embedding(inputs)
        print(f"  ‚Üí After embedding: shape {x.shape}") if training else None

        # LSTM layers: Capture temporal patterns
        x = self.lstm1(x, training=training)
        x = self.dropout1(x, training=training)
        print(f"  ‚Üí After LSTM 1: shape {x.shape}") if training else None

        x = self.lstm2(x, training=training)
        x = self.dropout2(x, training=training)
        print(f"  ‚Üí After LSTM 2: shape {x.shape}") if training else None

        x = self.lstm3(x, training=training)
        x = self.dropout3(x, training=training)
        print(f"  ‚Üí After LSTM 3: shape {x.shape}") if training else None

        # Dense layers: Final prediction
        x = self.dense1(x)
        output = self.output_layer(x)
        print(f"  ‚Üí Final output: shape {output.shape}") if training else None

        return output

model = StockRNN(input_shape=(seq_length, len(features)), lstm_units=50, embedding_dim=32)

_ = model(X_train[:1])

print("\nüìê Model Architecture:")
model.summary()

print(f"\nüî¢ Total parameters: {model.count_params():,}")


# ========== STEP 3: LOSS FUNCTION ==========
print("\n" + "="*70)
print("STEP 3: LOSS FUNCTION DEFINITION")
print("="*70)

print("\nüìâ Loss Function: Mean Squared Error (MSE)")
print("   Formula: MSE = (1/n) * Œ£(y_true - y_pred)¬≤")
print("   - Measures average squared difference between predictions and actual values")
print("   - Lower MSE = better predictions")

print("\nüéØ Optimizer: Adam (Adaptive Moment Estimation)")
print("   - Learning rate: 0.001")
print("   - Automatically adjusts learning rate during training")
print("   - Handles backward pass and weight updates")

# Define loss function (Mean Squared Error)
loss_fn = keras.losses.MeanSquaredError()

# Define optimizer (handles backward pass)
optimizer = keras.optimizers.Adam(learning_rate=0.001)

print("\n" + "="*70)
print("STEP 4: TRAINING WITH FORWARD & BACKWARD PASSES")
print("="*70)

# Metrics tracking
train_losses = []
val_losses = []
epoch_details = []

@tf.function
def train_step(x_batch, y_batch):
    """
    Single training step with explicit forward and backward pass
    """
    # FORWARD PASS
    with tf.GradientTape() as tape:
        # 1. Make predictions
        predictions = model(x_batch, training=True)

        # 2. Calculate loss
        loss = loss_fn(y_batch, predictions)

    # BACKWARD PASS
    # 3. Calculate gradients (how much each weight contributed to the loss)
    gradients = tape.gradient(loss, model.trainable_variables)

    # 4. Update weights using gradients
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    return loss, predictions

@tf.function
def val_step(x_batch, y_batch):
    """Validation step (forward pass only, no weight updates)"""
    predictions = model(x_batch, training=False)
    loss = loss_fn(y_batch, predictions)
    return loss, predictions

# Training parameters
epochs = 50
batch_size = 32
patience = 10
best_val_loss = float('inf')
patience_counter = 0

print(f"\nüöÄ Starting training for {epochs} epochs...")
print(f"Batch size: {batch_size}")
print(f"Early stopping patience: {patience}")

@tf.function
def test_step(x_batch, y_batch):
    """Test step (forward pass only, no weight updates)"""
    predictions = model(x_batch, training=False)
    loss = loss_fn(y_batch, predictions)
    return loss, predictions

best_test_loss = float('inf')  # Changed from best_train_loss
train_losses = []
test_losses = []

for epoch in range(epochs):

    print(f"\n{'='*70}")
    print(f"EPOCH {epoch + 1}/{epochs}")
    print(f"{'='*70}")

    # Training phase
    train_loss_batch = []
    num_batches = len(X_train) // batch_size

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = start_idx + batch_size

        x_batch = X_train[start_idx:end_idx]
        y_batch = y_train[start_idx:end_idx]

        # FORWARD & BACKWARD PASS
        if batch_idx == 0 and epoch == 0:
            print("\nüîÑ FORWARD PASS (First batch, first epoch):")

        loss, _ = train_step(x_batch, y_batch)
        train_loss_batch.append(loss.numpy())

        if batch_idx == 0 and epoch == 0:
            print("\n‚¨ÖÔ∏è BACKWARD PASS:")
            print("  ‚Üí Calculating gradients...")
            print("  ‚Üí Updating weights with optimizer...")

    avg_train_loss = np.mean(train_loss_batch)

    # Test phase evaluation
    test_loss_batch = []
    num_test_batches = len(X_test) // batch_size

    for batch_idx in range(num_test_batches):
        start_idx = batch_idx * batch_size
        end_idx = start_idx + batch_size

        x_batch = X_test[start_idx:end_idx]
        y_batch = y_test[start_idx:end_idx]

        loss, _ = test_step(x_batch, y_batch)
        test_loss_batch.append(loss.numpy())

    avg_test_loss = np.mean(test_loss_batch)

    # Store metrics
    train_losses.append(avg_train_loss)
    test_losses.append(avg_test_loss)

    print(f"\nüìä Results:")
    print(f"Training Loss: {avg_train_loss:.6f}")
    print(f"Test Loss:     {avg_test_loss:.6f}")

    # Early stopping check using test_loss
    if avg_test_loss < best_test_loss:
        best_test_loss = avg_test_loss
        patience_counter = 0
        model.save_weights('best_model_weights.weights.h5')

    else:
        patience_counter += 1
        print(f"‚è≥ Patience: {patience_counter}/{patience}")

    if patience_counter >= patience:
        print(f"\nüõë Early stopping triggered at epoch {epoch + 1}")
        break

model.load_weights('best_model_weights.weights.h5')

# ========== STEP 5: EVALUATION ==========
print("\n" + "="*70)
print("STEP 5: MODEL EVALUATION")
print("="*70)

# Make predictions
print("\nüîÆ Making predictions on test set...")
test_predictions = model.predict(X_test, batch_size=batch_size, verbose=0)
train_predictions = model.predict(X_train, batch_size=batch_size, verbose=0)

# Inverse transform to original scale
def inverse_transform_predictions(predictions, scaler):
    dummy = np.zeros((len(predictions), scaler.n_features_in_))
    dummy[:, 0] = predictions.flatten()
    return scaler.inverse_transform(dummy)[:, 0]

test_pred_original = inverse_transform_predictions(test_predictions, scaler)
train_pred_original = inverse_transform_predictions(train_predictions, scaler)

# Get actual values
train_actual = stock_df['Close'].values[seq_length:seq_length+len(train_predictions)]
test_actual = stock_df['Close'].values[seq_length+len(train_predictions):seq_length+len(train_predictions)+len(test_predictions)]

# Calculate metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

train_rmse = np.sqrt(mean_squared_error(train_actual, train_pred_original))
test_rmse = np.sqrt(mean_squared_error(test_actual, test_pred_original))
train_mae = mean_absolute_error(train_actual, train_pred_original)
test_mae = mean_absolute_error(test_actual, test_pred_original)
test_r2 = r2_score(test_actual, test_pred_original)

print(f"\nüìà Performance Metrics:")
print(f"{'='*50}")
print(f"{'Metric':<20} {'Train':<15} {'Test':<15}")
print(f"{'='*50}")
print(f"{'RMSE':<20} ${train_rmse:<14.4f} ${test_rmse:<14.4f}")
print(f"{'MAE':<20} ${train_mae:<14.4f} ${test_mae:<14.4f}")
print(f"{'R¬≤ Score':<20} {'-':<15} {test_r2:<14.4f}")
print(f"{'='*50}")

print("\nüìä Generating visualizations...")

fig = plt.figure(figsize=(15, 12))

# Plot 1: Loss curves
ax1 = plt.subplot(3, 1, 1)
plt.plot(train_losses, label='Training Loss', linewidth=2)
plt.title(f'{ticker_to_predict} - Training History (Forward & Backward Passes)', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True, alpha=0.3)

ax2 = plt.subplot(3, 1, 2)
train_dates = stock_df['Date'].values[seq_length:seq_length+len(train_predictions)]
plt.plot(train_dates, train_actual, label='Actual Price', alpha=0.7, linewidth=2)
plt.plot(train_dates, train_pred_original, label='Predicted Price', alpha=0.7, linewidth=2)
plt.title(f'{ticker_to_predict} - Training Set Predictions', fontsize=14, fontweight='bold')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

ax3 = plt.subplot(3, 1, 3)
test_dates = stock_df['Date'].values[seq_length+len(train_predictions):seq_length+len(train_predictions)+len(test_predictions)]
plt.plot(test_dates, test_actual, label='Actual Price', alpha=0.7, linewidth=2)
plt.plot(test_dates, test_pred_original, label='Predicted Price', alpha=0.7, linewidth=2)
plt.fill_between(test_dates, test_actual, test_pred_original, alpha=0.2)
plt.title(f'{ticker_to_predict} - Test Set Predictions (Unseen Data)', fontsize=14, fontweight='bold')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()





"""**SECOND TRY**"""

# Optimized hyperparameters
ticker_to_predict = 'MSI'
seq_length = 30  # Reduced from 60
epochs = 100  # Increased
batch_size = 16  # Reduced for better gradient updates
patience = 20  # More patience
learning_rate = 0.0005  # Adjusted learning rate

stock_df = df[df['Ticker'] == ticker_to_predict].copy()
stock_df = stock_df.sort_values('Date').reset_index(drop=True)

features = ['Close', 'Open', 'High', 'Low', 'Volume']
data = stock_df[features].values

print(f"Total data points for {ticker_to_predict}: {len(data)}")

# Normalize
scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data)

# Create sequences
X, y = create_sequences(data_scaled, seq_length)

# Split 80/20
train_size = int(len(X) * 0.8)
X_train = X[:train_size]
y_train = y[:train_size]
X_test = X[train_size:]
y_test = y[train_size:]

print(f"Training samples: {len(X_train)}")
print(f"Test samples: {len(X_test)}")

class OptimizedStockRNN(keras.Model):
    def __init__(self, input_shape, lstm_units=32):
        super(OptimizedStockRNN, self).__init__()

        # Simpler architecture - NO embedding
        self.lstm1 = LSTM(lstm_units, return_sequences=True, name='lstm_1')
        self.dropout1 = Dropout(0.3, name='dropout_1')

        self.lstm2 = LSTM(lstm_units, return_sequences=False, name='lstm_2')
        self.dropout2 = Dropout(0.3, name='dropout_2')

        self.dense1 = Dense(16, activation='relu', name='dense_1')
        self.output_layer = Dense(1, name='output')

    def call(self, inputs, training=False):
        x = self.lstm1(inputs, training=training)
        x = self.dropout1(x, training=training)

        x = self.lstm2(x, training=training)
        x = self.dropout2(x, training=training)

        x = self.dense1(x)
        output = self.output_layer(x)
        return output

# Build model
model = OptimizedStockRNN(input_shape=(seq_length, len(features)), lstm_units=32)
_ = model(X_train[:1])

print("‚úÖ Optimized model built")
model.summary()

loss_fn = keras.losses.MeanSquaredError()
optimizer = keras.optimizers.Adam(learning_rate=learning_rate)

@tf.function
def train_step(x_batch, y_batch):
    with tf.GradientTape() as tape:
        predictions = model(x_batch, training=True)
        loss = loss_fn(y_batch, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss, predictions

@tf.function
def test_step(x_batch, y_batch):
    predictions = model(x_batch, training=False)
    loss = loss_fn(y_batch, predictions)
    return loss, predictions

print("‚úÖ Training functions defined")

train_losses = []
test_losses = []
best_test_loss = float('inf')
patience_counter = 0

print(f"\nüöÄ Training {ticker_to_predict} with optimized settings...")
print(f"Epochs: {epochs}, Batch size: {batch_size}, Learning rate: {learning_rate}")

for epoch in range(epochs):

    # Training phase
    train_loss_batch = []
    num_batches = len(X_train) // batch_size

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = start_idx + batch_size

        x_batch = X_train[start_idx:end_idx]
        y_batch = y_train[start_idx:end_idx]

        loss, _ = train_step(x_batch, y_batch)
        train_loss_batch.append(loss.numpy())

    avg_train_loss = np.mean(train_loss_batch)

    # Test phase
    test_loss_batch = []
    num_test_batches = len(X_test) // batch_size

    for batch_idx in range(num_test_batches):
        start_idx = batch_idx * batch_size
        end_idx = start_idx + batch_size

        x_batch = X_test[start_idx:end_idx]
        y_batch = y_test[start_idx:end_idx]

        loss, _ = test_step(x_batch, y_batch)
        test_loss_batch.append(loss.numpy())

    avg_test_loss = np.mean(test_loss_batch)

    # Store metrics
    train_losses.append(avg_train_loss)
    test_losses.append(avg_test_loss)

    # Print every 5 epochs
    if (epoch + 1) % 5 == 0:
        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.6f}, Test Loss: {avg_test_loss:.6f}")

    # Early stopping
    if avg_test_loss < best_test_loss:
        best_test_loss = avg_test_loss
        patience_counter = 0
        model.save_weights('best_model_weights.weights.h5')
        if (epoch + 1) % 5 == 0:
            print(f"  ‚úÖ New best model!")
    else:
        patience_counter += 1

    if patience_counter >= patience:
        print(f"\nüõë Early stopping at epoch {epoch + 1}")
        break

# Load best weights
model.load_weights('best_model_weights.weights.h5')
print(f"\n‚úÖ Training complete! Best test loss: {best_test_loss:.6f}")

# Make predictions
train_predictions = model.predict(X_train, batch_size=batch_size, verbose=0)
test_predictions = model.predict(X_test, batch_size=batch_size, verbose=0)

# Inverse transform
def inverse_transform_predictions(predictions, scaler):
    dummy = np.zeros((len(predictions), scaler.n_features_in_))
    dummy[:, 0] = predictions.flatten()
    return scaler.inverse_transform(dummy)[:, 0]

train_pred_original = inverse_transform_predictions(train_predictions, scaler)
test_pred_original = inverse_transform_predictions(test_predictions, scaler)

train_actual = stock_df['Close'].values[seq_length:seq_length+len(train_predictions)]
test_actual = stock_df['Close'].values[seq_length+len(train_predictions):seq_length+len(train_predictions)+len(test_predictions)]

# Metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
test_rmse = np.sqrt(mean_squared_error(test_actual, test_pred_original))
test_mae = mean_absolute_error(test_actual, test_pred_original)
test_r2 = r2_score(test_actual, test_pred_original)

print(f"\nüìà Performance:")
print(f"Test RMSE: ${test_rmse:.2f}")
print(f"Test MAE:  ${test_mae:.2f}")
print(f"Test R¬≤:   {test_r2:.4f}")

# Plot
fig = plt.figure(figsize=(15, 10))

# Loss curves
plt.subplot(3, 1, 1)
plt.plot(train_losses, label='Training Loss', linewidth=2)
plt.plot(test_losses, label='Test Loss', linewidth=2)
plt.title('Training History', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Training predictions
plt.subplot(3, 1, 2)
train_dates = stock_df['Date'].values[seq_length:seq_length+len(train_predictions)]
plt.plot(train_dates, train_actual, label='Actual', alpha=0.7, linewidth=2)
plt.plot(train_dates, train_pred_original, label='Predicted', alpha=0.7, linewidth=2)
plt.title('Training Set', fontsize=14, fontweight='bold')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

# Test predictions
plt.subplot(3, 1, 3)
test_dates = stock_df['Date'].values[seq_length+len(train_predictions):seq_length+len(train_predictions)+len(test_predictions)]
plt.plot(test_dates, test_actual, label='Actual', alpha=0.7, linewidth=2)
plt.plot(test_dates, test_pred_original, label='Predicted', alpha=0.7, linewidth=2)
plt.title('Test Set', fontsize=14, fontweight='bold')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

